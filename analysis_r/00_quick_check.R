# SolarAI_Data_Analysis.R
# Analysis of Synthetic Dataset for Solar-Powered AI Healthcare Units
# Date: May 12, 2025
# Purpose: Analyze synthetic patient data for training TinyLLaMA models to generate SOAP notes
#          in a solar-powered AI unit for rural healthcare. Includes data preprocessing,
#          bias analysis, and fine-tuning rationale for public transparency.

# Load required libraries
library(jsonlite)
library(dplyr)
library(ggplot2)
library(tidyr)

# Set working directory (adjust as needed)
# setwd("path/to/your/project")

# Create output directory for data and plots
dir.create("data", showWarnings = FALSE)

# 1. Introduction (Commented for Documentation)
# This script analyzes a synthetic dataset of 1,032 patient records to train four
# TinyLLaMA models for generating Subjective, Objective, Assessment, and Plan (SOAP)
# notes in a solar-powered, on-premises AI unit. The dataset includes demographics,
# chief complaints, transcriptions, and SOAP notes, with an augmented dataset providing
# paraphrased SOAP sections. The analysis ensures transparency, evaluates biases, and
# prepares data for fine-tuning, supporting affordable AI solutions for rural healthcare.

# 2. Load Datasets
# Primary dataset: synthetic_patients_with_soap.jsonl
patients <- stream_in(file("data/synthetic_patients_with_soap.jsonl"), verbose = FALSE)

# Augmented dataset: soap_augmented.jsonl (paraphrased SOAP sections)
augmented <- stream_in(file("data/soap_augmented.jsonl"), verbose = FALSE)

# Display dataset structure
cat("Structure of primary dataset:\n")
str(patients, max.level = 1)
cat("\nStructure of augmented dataset:\n")
str(augmented, max.level = 1)

# 3. Demographic Distribution
# Summarize gender, age, smoker, and drinker distributions
demo_summary <- patients %>%
  summarise(
    Gender = paste(names(table(gender)), collapse = ", "),
    Age_Range = paste(range(age), collapse = " to "),
    Smoker = paste(names(table(smoker)), collapse = ", "),
    Drinker = paste(names(table(drinker)), collapse = ", ")
  )

cat("\nDemographic Summary:\n")
print(demo_summary)

# Plot age distribution
png("data/age_distribution.png", width = 800, height = 600)
ggplot(patients, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.6) +
  labs(title = "Age Distribution of Synthetic Patients", x = "Age", y = "Count") +
  theme_minimal()
dev.off()
cat("Saved age distribution plot to data/age_distribution.png\n")

# 4. Chief Complaints
# Summarize top 10 chief complaints
cc_freq <- patients %>%
  count(chief_complaint, sort = TRUE) %>%
  slice_head(n = 10)

cat("\nTop 10 Chief Complaints:\n")
print(cc_freq)

# Plot top 10 chief complaints
png("data/chief_complaints.png", width = 800, height = 600)
ggplot(cc_freq, aes(x = reorder(chief_complaint, -n), y = n)) +
  geom_bar(stat = "identity", fill = "green", alpha = 0.6) +
  labs(title = "Top 10 Chief Complaints", x = "Complaint", y = "Frequency") +
  theme_minimal() +
  coord_flip()
dev.off()
cat("Saved chief complaints plot to data/chief_complaints.png\n")

# 5. SOAP Notes Summary
# Summarize augmented SOAP section formats
aug_format <- augmented %>%
  count(format, section)

cat("\nAugmented SOAP Section Formats:\n")
print(aug_format)

# 6. Data Preprocessing
# Split augmented dataset into four subsets for Subjective, Objective, Assessment, Plan
subjective <- augmented %>% filter(section == "subjective") %>% select(row_id, text)
objective <- augmented %>% filter(section == "objective") %>% select(row_id, text)
assessment <- augmented %>% filter(section == "assessment") %>% select(row_id, text)
plan <- augmented %>% filter(section == "plan") %>% select(row_id, text)

# Save preprocessed datasets
write_json(subjective, "data/subjective.jsonl", auto_unbox = TRUE, pretty = TRUE)
write_json(objective, "data/objective.jsonl", auto_unbox = TRUE, pretty = TRUE)
write_json(assessment, "data/assessment.jsonl", auto_unbox = TRUE, pretty = TRUE)
write_json(plan, "data/plan.jsonl", auto_unbox = TRUE, pretty = TRUE)
cat("Saved preprocessed datasets to data/subjective.jsonl, data/objective.jsonl, data/assessment.jsonl, data/plan.jsonl\n")

# 7. Bias and Uncertainties Analysis
# 7.1 Demographic Bias
# The synthetic dataset uses random assignments for gender, age, smoker, and drinker status.
# Uneven distributions may bias model predictions. The age histogram (data/age_distribution.png)
# suggests a uniform spread, but real-world validation is needed to ensure representativeness.

# 7.2 Synthetic Data Limitations
# Generated by GPT-4o, the dataset may lack real-world variability, potentially introducing
# artifacts like standardized language. The augmented dataset's multiple styles (simple, bullets,
# narrative, brev_bp, detailed) mitigate this by increasing linguistic diversity.

# 7.3 Uncertainties
# - Model Performance: Mockup data indicate 95% transcription accuracy and 90% diagnostic
#   precision, but real-world conditions (e.g., noisy audio, diverse accents) may reduce performance.
# - Generalization: Models may overfit to synthetic patterns, requiring real-world testing.
# - Hardware Constraints: The Raspberry Pi 5's limited resources necessitate 4-bit quantization,
#   which may slightly lower accuracy.

cat("\nBias and Uncertainties Notes:\n")
cat("Demographic bias: Random assignments may skew predictions; validate with real data.\n")
cat("Synthetic data: May lack real-world variability; mitigated by augmentation.\n")
cat("Uncertainties: Real-world performance, overfitting, and quantization effects need testing.\n")

# 8. Fine-Tuning Rationale
# 8.1 Model Selection
# TinyLLaMA (1.1B parameters) is selected for its efficiency on low-power devices like the
# Raspberry Pi 5. Quantized to 4-bit, it requires ~2GB memory, unlike larger models (e.g.,
# LLaMA-3.1-8B, ~4GB).

# 8.2 Fine-Tuning Process
# Four TinyLLaMA models are trained, one per SOAP section, using Hugging Face Transformers:
# - Datasets: subjective.jsonl, objective.jsonl, assessment.jsonl, plan.jsonl
# - Training: On a high-performance server with 3 epochs, batch size 8, learning rate 2e-5,
#   AdamW optimizer
# - Quantization: 4-bit using bitsandbytes library
# - Deployment: Via ONNX Runtime on Ubuntu 24.04 LTS

# 8.3 Why Four Models?
# Specialized models improve accuracy and reduce computational load by focusing on distinct
# tasks (e.g., Subjective for history, Assessment for diagnostics).

cat("\nFine-Tuning Notes:\n")
cat("Model: TinyLLaMA (1.1B parameters), quantized to 4-bit for Raspberry Pi 5.\n")
cat("Process: Train four models on subjective, objective, assessment, plan datasets.\n")
cat("Rationale: Specialization enhances accuracy and efficiency on low-power hardware.\n")

# 9. Ethical Considerations
# - Data Privacy: Synthetic data avoids real patient information; deployed systems use AES-256
#   encryption for HIPAA compliance.
# - Bias Mitigation: Regular audits of model outputs to address biases, especially for
#   underrepresented groups.
# - Clinician Oversight: AI outputs are preliminary, requiring clinician validation.
# - Transparency: This script and datasets are public for reproducibility and critique.

cat("\nEthical Considerations:\n")
cat("Privacy: Synthetic data; deploy with AES-256 encryption.\n")
cat("Bias: Audit outputs for fairness.\n")
cat("Oversight: Clinician validation required.\n")
cat("Transparency: Public script and data for critique.\n")

# 10. Conclusion
# This analysis supports the development of solar-powered AI units for rural healthcare by
# preparing synthetic data for TinyLLaMA fine-tuning. The dataset's structure, biases, and
# preprocessing are transparently documented. Real-world trials will validate performance,
# and public feedback is invited to refine the approach.

cat("\nConclusion:\n")
cat("Synthetic dataset supports TinyLLaMA training for SOAP note generation.\n")
cat("Transparent analysis invites critique to improve rural healthcare AI.\n")

# 11. Reproducibility
# To replicate:
# 1. Install R and packages: jsonlite, dplyr, ggplot2, tidyr
# 2. Place synthetic_patients_with_soap.jsonl and soap_augmented.jsonl in data/
# 3. Run this script in RStudio
# 4. For fine-tuning, use Hugging Face Transformers with preprocessed JSONL files

cat("\nReproducibility Instructions:\n")
cat("1. Install R, jsonlite, dplyr, ggplot2, tidyr.\n")
cat("2. Place datasets in data/ directory.\n")
cat("3. Run script in RStudio.\n")
cat("4. Fine-tune with Hugging Face Transformers.\n")

# End of script
cat("Analysis complete. Outputs saved in data/ directory.\n")

